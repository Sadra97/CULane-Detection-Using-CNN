{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTcNiK5m_gyF",
        "outputId": "6b749542-6939-4ac1-b980-83848e534b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Import necessary items\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tarfile\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, UpSampling2D, Dense, Flatten, Reshape\n",
        "from keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "# Mounting  and opening dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnTl1RxGNmbt",
        "outputId": "7923968c-1f11-4b17-ee09-608765b5c366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 9999/18233 [15:00<12:21, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8838, 118, 328, 1)\n",
            "(8838, 118, 328, 3)\n"
          ]
        }
      ],
      "source": [
        "# Building list of datasets dir\n",
        "myimages = [] #list of images \n",
        "myannot = [] #list of annotations\n",
        "dirMain = os.listdir('/content/driver_161_90frame') #list of directory files\n",
        "for folder in sorted(dirMain):\n",
        "  counter_jpg=0\n",
        "  counter_txt=0\n",
        "  for file in sorted(os.listdir('/content/driver_161_90frame/' + folder)):\n",
        "      if '.jpg' in file:\n",
        "          image_name=file.split(\".\")\n",
        "          image1_name=image_name[0]\n",
        "          text1_name = image1_name  + '.lines.txt'\n",
        "          file1_name=file\n",
        "          counter_jpg=counter_jpg + 1\n",
        "          for file in sorted(os.listdir('/content/driver_161_90frame/' + folder)):\n",
        "             if (text1_name in file):               \n",
        "               myannot.append('/content/driver_161_90frame/' + folder + '/' + file)\n",
        "               myimages.append('/content/driver_161_90frame/' + folder + '/' + file1_name)\n",
        "               counter_txt=counter_txt + 1\n",
        "############################################################################\n",
        "# Building images tensor\n",
        "N_train_image = 10000\n",
        "train_images = []\n",
        "from tqdm import tqdm\n",
        "x_pixels = 590\n",
        "y_pixels = 1640\n",
        "train_labels = []\n",
        "for j in tqdm(range(len(myannot))):\n",
        "    lines = []\n",
        "    with open(myannot[j], 'r') as f:\n",
        "        data = f.readlines()\n",
        "        if os.stat(myannot[j]).st_size == 0:\n",
        "          pass\n",
        "        else:\n",
        "          lines = [line.split() for line in data]\n",
        "          lines = [[int(float(i)) for i in line] for line in lines]\n",
        "          lines = [[(line[i], line[i + 1]) for i in range(0, len(line), 2)] for line in lines]\n",
        "          lines = [line for line in lines if len(line) != 1]\n",
        "          new_lines = [*lines]\n",
        "          lines = []\n",
        "          for line in new_lines:\n",
        "              try:\n",
        "                  x1 = line[0][0]\n",
        "                  y1 = line[0][1]\n",
        "                  x2 = line[-1][0]\n",
        "                  y2 = line[-1][1]\n",
        "\n",
        "                  m = (y2 - y1) / (x2 - x1)\n",
        "                  b = y1 - m * x1\n",
        "\n",
        "                  lines.append((x1, y1, x2, y2, m, b))\n",
        "              except:\n",
        "                  continue\n",
        "          label = np.zeros((5000, 5000))\n",
        "          for line in lines:\n",
        "              x1, y1, x2, y2, m, b = line\n",
        "              x = np.arange(x1, x2, 1 if x2 > x1 else -1)\n",
        "              y = m * x + b\n",
        "              y = np.array(y, dtype=int)\n",
        "              label[y, x] = 255\n",
        "              # print(tf.reduce_max(label))\n",
        "          label = label[:590, :1640]\n",
        "          label = np.array(Image.fromarray(label).resize((328, 118)))\n",
        "          train_labels.append(label)\n",
        "          myimages[j] = plt.imread(myimages[j])  #opencv2.resize or pilow\n",
        "          scale_percent = 50 # percent of original size\n",
        "          width = int(myimages[j].shape[1] * scale_percent / 100)\n",
        "          height = int(myimages[j].shape[0] * scale_percent / 100)\n",
        "          dim = (width, height)      \n",
        "          myimages[j] = np.array(Image.fromarray(myimages[j]).resize((328, 118)))\n",
        "          train_images.append(myimages[j])          \n",
        "    if j == N_train_image-1:\n",
        "      break\n",
        "train_labels = np.array(train_labels)\n",
        "train_labels = np.expand_dims(train_labels, -1)\n",
        "train_images = np.array(train_images)\n",
        "train_images = train_images[:,54:,:,:]\n",
        "train_labels = train_labels[:,54:,:,:]\n",
        "print(train_labels.shape)\n",
        "print(train_images.shape)\n",
        "label_max = tf.reduce_max(train_labels)\n",
        "train_labels = train_labels/label_max\n",
        "train_labels = train_images/255\n",
        "print(tf.reduce_max(train_labels))\n",
        "print(tf.reduce_max(train_images))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building test data\n",
        "myimages_test = [] #list of images test\n",
        "myannot_test = [] #list of annotations test\n",
        "dirMain = os.listdir('/content/driver_37_30frame') #list of directory files\n",
        "for folder in sorted(dirMain):\n",
        "  counter_jpg=0\n",
        "  counter_txt=0\n",
        "  for file in sorted(os.listdir('/content/driver_37_30frame/' + folder)):\n",
        "      if '.jpg' in file:\n",
        "          image_name=file.split(\".\")\n",
        "          image1_name=image_name[0]\n",
        "          text1_name = image1_name  + '.lines.txt'\n",
        "          file1_name=file\n",
        "          counter_jpg=counter_jpg + 1\n",
        "          for file in sorted(os.listdir('/content/driver_37_30frame/' + folder)):\n",
        "             if (text1_name in file):               \n",
        "               myannot_test.append('/content/driver_37_30frame/' + folder + '/' + file)\n",
        "               myimages_test.append('/content/driver_37_30frame/' + folder + '/' + file1_name)\n",
        "############################################################################\n",
        "# Building test image and annot tensor\n",
        "N_test_image = 500\n",
        "test_images = []\n",
        "from tqdm import tqdm\n",
        "x_pixels = 590\n",
        "y_pixels = 1640\n",
        "test_labels = []\n",
        "for j in tqdm(range(len(myannot_test))):\n",
        "    lines = []\n",
        "    with open(myannot_test[j], 'r') as f:\n",
        "        data = f.readlines()\n",
        "        if os.stat(myannot_test[j]).st_size == 0:\n",
        "          pass\n",
        "        else:\n",
        "          lines = [line.split() for line in data]\n",
        "          lines = [[int(float(i)) for i in line] for line in lines]\n",
        "          lines = [[(line[i], line[i + 1]) for i in range(0, len(line), 2)] for line in lines]\n",
        "          lines = [line for line in lines if len(line) != 1]\n",
        "          new_lines = [*lines]\n",
        "          lines = []\n",
        "          for line in new_lines:\n",
        "              try:\n",
        "                  x1 = line[0][0]\n",
        "                  y1 = line[0][1]\n",
        "                  x2 = line[-1][0]\n",
        "                  y2 = line[-1][1]\n",
        "\n",
        "                  m = (y2 - y1) / (x2 - x1)\n",
        "                  b = y1 - m * x1\n",
        "\n",
        "                  lines.append((x1, y1, x2, y2, m, b))\n",
        "              except:\n",
        "                  continue\n",
        "          label = np.zeros((5000, 5000))\n",
        "          for line in lines:\n",
        "              x1, y1, x2, y2, m, b = line\n",
        "              x = np.arange(x1, x2, 1 if x2 > x1 else -1)\n",
        "              y = m * x + b\n",
        "              y = np.array(y, dtype=int)\n",
        "              label[y, x] = 255\n",
        "          label = label[:590, :1640]\n",
        "          label = np.array(Image.fromarray(label).resize((328, 118)))\n",
        "          test_labels.append(label)\n",
        "          myimages_test[j] = plt.imread(myimages_test[j])  #opencv2.resize or pilow\n",
        "          scale_percent = 50 # percent of original size\n",
        "          width = int(myimages_test[j].shape[1] * scale_percent / 100)\n",
        "          height = int(myimages_test[j].shape[0] * scale_percent / 100)\n",
        "          dim = (width, height)      \n",
        "          myimages_test[j] = np.array(Image.fromarray(myimages_test[j]).resize((328, 118)))\n",
        "          test_images.append(myimages_test[j])          \n",
        "    if j == N_test_image-1:\n",
        "      break\n",
        "test_labels = np.array(test_labels)\n",
        "test_labels = np.expand_dims(test_labels, -1)\n",
        "test_images = np.array(test_images)\n",
        "test_images = test_images[:,54:,:,:]\n",
        "test_labels = test_labels[:,54:,:,:]\n",
        "print(test_labels.shape)\n",
        "print(test_images.shape)\n",
        "label_max = tf.reduce_max(test_labels)\n",
        "test_labels = test_labels/label_max\n",
        "test_labels = test_images/255\n",
        "print(tf.reduce_max(test_labels))\n",
        "print(tf.reduce_max(test_images))"
      ],
      "metadata": {
        "id": "dr9T37gqjITp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97EEWOEDDksv"
      },
      "outputs": [],
      "source": [
        "def create_model1(input_shape, pool_size):\n",
        "    # Create the actual neural network here\n",
        "    model = Sequential()\n",
        "    # Normalizes incoming inputs. First layer needs the input shape to work\n",
        "    model.add(BatchNormalization(input_shape=input_shape))\n",
        "\n",
        "    # Below layers were re-named for easier reading of model summary; this not necessary\n",
        "    # Conv Layer 1\n",
        "    model.add(Conv2D(8, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Conv1'))\n",
        "\n",
        "    # Conv Layer 2\n",
        "    model.add(Conv2D(16, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Conv2'))\n",
        "\n",
        "    # Pooling 1\n",
        "    model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "    # Conv Layer 3\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Conv4'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 4\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Conv5'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 5\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Conv6'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Layer 6\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Conv7'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Upsample 1\n",
        "    model.add(UpSampling2D(size=pool_size))\n",
        "\n",
        "    # Deconv 1\n",
        "    model.add(Conv2DTranspose(64, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Deconv1'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 2\n",
        "    model.add(Conv2DTranspose(64, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Deconv2'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 3\n",
        "    model.add(Conv2DTranspose(32, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Deconv3'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 4\n",
        "    model.add(Conv2DTranspose(32, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Deconv4'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Deconv 6\n",
        "    model.add(Conv2DTranspose(16, (3, 3), padding='same', strides=(1,1), activation = 'relu', name = 'Deconv6'))\n",
        "\n",
        "    # Final layer \n",
        "    model.add(Conv2DTranspose(1, (3, 3), padding='same', strides=(1,1), activation = 'sigmoid', name = 'Final'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P1r1G2BDq7O"
      },
      "outputs": [],
      "source": [
        "#def main():\n",
        "\n",
        "# Make into arrays as the neural network wants these\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Shuffle images along with their labels, then split into training/validation sets\n",
        "train_images, train_labels = shuffle(train_images, train_labels)\n",
        "\n",
        "# Test size may be 10% or 5%\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.05)\n",
        "\n",
        "# Batch size, epochs and pool size below are all paramaters to fiddle with for optimization\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "pool_size = (2, 2)\n",
        "input_shape = X_train.shape[1:]\n",
        "# Create the neural network\n",
        "model = create_model1(input_shape, pool_size)\n",
        "\n",
        "# Using a generator to help the model use less data\n",
        "# Channel shifts help with shadows slightly\n",
        "datagen = ImageDataGenerator(channel_shift_range=0.2)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Compiling and training the model\n",
        "opt = keras.optimizers.SGD(learning_rate=0.0005)\n",
        "model.compile(optimizer='Adam', loss='bce', metrics='accuracy')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, verbose=1, validation_data=(X_val, y_val)) \n",
        "# Freeze layers since training is done\n",
        "model.trainable = False\n",
        "model.compile(optimizer='Adam', loss='bce')\n",
        "\n",
        "# Show summary of model\n",
        "model.summary()\n",
        "\n",
        "hist = history.history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_images,test_labels, verbose=1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ja1nf58FrQUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0TZdwDsDyM2"
      },
      "outputs": [],
      "source": [
        "def plot_history():\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(range(1, len(hist['loss']) + 1), hist['loss'],label='train error')\n",
        "    plt.plot(range(1, len(hist['loss']) + 1), hist['val_loss'], label='val error')\n",
        "    plt.legend()\n",
        "plot_history()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 450\n",
        "print(np.expand_dims(test_images[idx], 0).shape)\n",
        "prediction = model.predict(np.expand_dims(test_images[idx], 0)).squeeze()\n",
        "prediction[prediction < 0.05] = 0\n",
        "prediction[prediction > 0.05] = 1\n",
        "plt.figure(dpi=150)\n",
        "plt.imshow(test_images[idx]+ test_labels[idx])\n",
        "plt.show()\n",
        "plt.figure(dpi=150)\n",
        "plt.imshow(test_images[idx] + np.expand_dims(prediction, -1)*5)\n",
        "plt.show()\n",
        "plt.figure(dpi=150)\n",
        "plt.imshow(prediction+ test_labels[idx].squeeze()) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YRbX-29hk_DQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}